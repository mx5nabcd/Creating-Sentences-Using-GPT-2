{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 출처\n",
    "# https://wikidocs.net/217618\n",
    "# clone coding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:04.149719Z",
     "start_time": "2024-09-21T11:22:04.145696800Z"
    }
   },
   "id": "ad32ae1afeb8efa0",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2LMHeadModel"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:06.692962200Z",
     "start_time": "2024-09-21T11:22:04.149719Z"
    }
   },
   "id": "initial_id",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ogoj9\\anaconda3\\envs\\nlpwiki2\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:08.964873100Z",
     "start_time": "2024-09-21T11:22:06.692962200Z"
    }
   },
   "id": "9d1769e7e837c3ef",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sent = '근육이 커지기 위해서는'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:08.967869400Z",
     "start_time": "2024-09-21T11:22:08.963318300Z"
    }
   },
   "id": "f2fbfb262e7e6481",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33245, 10114, 12748, 11357]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(sent, return_tensors='pt')\n",
    "print(input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:09.007384100Z",
     "start_time": "2024-09-21T11:22:08.967869400Z"
    }
   },
   "id": "103719d9cea5f42b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33245, 10114, 12748, 11357, 23879, 39306, 9684, 7884, 10211, 15177, 26421, 387, 17339, 7889, 9908, 15768, 6903, 15386, 8146, 12923, 9228, 18651, 42600, 9564, 17764, 9033, 9199, 14441, 7335, 8704, 12557, 32030, 9510, 18595, 9025, 10571, 25741, 10599, 13229, 9508, 7965, 8425, 33102, 9122, 21240, 9801, 32106, 13579, 12442, 13235, 19430, 8022, 12972, 9566, 11178, 9554, 24873, 7198, 9391, 12486, 8711, 9346, 7071, 36736, 9693, 12006, 9038, 10279, 36122, 9960, 8405, 10826, 18988, 25998, 9292, 7671, 9465, 7489, 9277, 10137, 9677, 9248, 9912, 12834, 11488, 13417, 7407, 8428, 8137, 9430, 14222, 11356, 10061, 9885, 19265, 9377, 20305, 7991, 9178, 9648, 9133, 10021, 10138, 30315, 21833, 9362, 9301, 9685, 11584, 9447, 42129, 10124, 7532, 17932, 47123, 37544, 9355, 15632, 9124, 10536, 13530, 12204, 9184, 36152, 9673, 9788, 9029, 11764]\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids,\n",
    "                        max_length=128,\n",
    "                        repetition_penalty=2.0,\n",
    "                        use_cache=True)\n",
    "output_ids = output.numpy().tolist()[0]\n",
    "print(output_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:11.284049800Z",
     "start_time": "2024-09-21T11:22:08.985872300Z"
    }
   },
   "id": "73c87315e46b0c67",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'근육이 커지기 위해서는 무엇보다 규칙적인 생활습관이 중요하다.\\n특히, 아침식사는 단백질과 비타민이 풍부한 과일과 채소를 많이 섭취하는 것이 좋다.\\n또한 하루 30분 이상 충분한 수면을 취하는 것도 도움이 된다.\\n아침 식사를 거르지 않고 규칙적으로 운동을 하면 혈액순환에 도움을 줄 뿐만 아니라 신진대사를 촉진해 체내 노폐물을 배출하고 혈압을 낮춰준다.\\n운동은 하루에 10분 정도만 하는 게 좋으며 운동 후에는 반드시 스트레칭을 통해 근육량을 늘리고 유연성을 높여야 한다.\\n운동 후 바로 잠자리에 드는 것은 피해야 하며 특히 아침에 일어나면 몸이 피곤해지기 때문에 무리하게 움직이면 오히려 역효과가 날 수도 있다.\\n운동을'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:11.287538500Z",
     "start_time": "2024-09-21T11:22:11.285050500Z"
    }
   },
   "id": "d258edc136ce1278",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output = model(input_ids)\n",
    "\n",
    "# logits.shape == torch.Size([51200]). 즉, 총 단어 집합 크기만큼의 차원을 가지는 벡터.\n",
    "logits = output.logits[0, -1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:11.332999Z",
     "start_time": "2024-09-21T11:22:11.287538500Z"
    }
   },
   "id": "bc470a066d82859e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['무엇보다', '우선', '반드시', '피부', '무엇보다도']\n"
     ]
    }
   ],
   "source": [
    "top5 = torch.topk(logits, k=5)\n",
    "tokens = [tokenizer.decode(token_id) for token_id in top5.indices.tolist()]\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:11.333998200Z",
     "start_time": "2024-09-21T11:22:11.315117800Z"
    }
   },
   "id": "5702b0d79316c6c",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'근육이 커지기 위해서는 △가볍거나 무거운 옷의 △얇으면서도 따뜻하면 편하도록 만들어지는 것이 필요하다.\\n아름찬 느낌을 강조한 패커버가 인기이지만 이는 몸에 딱 달라붙기는 커리어 스타일러로서의 역할도 하고 마사지 등으로 땀'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(sent, return_tensors='pt')\n",
    "\n",
    "while len(input_ids[0]) < 50:\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    logits = output.logits[0, -1]\n",
    "    top5 = torch.topk(logits, k=30)\n",
    "    token_id = random.choice(top5.indices.tolist())\n",
    "    input_ids = torch.cat([input_ids, torch.tensor([[token_id]])], dim=1)\n",
    "\n",
    "tokenizer.decode(input_ids[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-21T11:22:31.033162400Z",
     "start_time": "2024-09-21T11:22:29.719777800Z"
    }
   },
   "id": "2cf362a34a41fcc7",
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
